%!TEX root = ../main.tex

\section{Analysis} % (fold)
\label{sec:analysis}

\subsection{Advantages and Disadvantages of Sonification} % (fold)
\label{sub:advantages_and_disadvantages_of_sonification}

Advantages of sonification are many. 
Not only can we represent data and learn patterns and cycles in said data, but we can also aid the disabled and assist workers in different fields with sound patterns and alarms (Section~\ref{sec:preanalysis}).
However, as with everything there are certain disadvantages of sonification. 
The disadvantages can all be lead from the same disadvantages that sound has. 
Sound can be frustrating or annoying for some users, as people are forced to impose their own taste and ideas subjectively on data. 
Sound is also incredibly hard to describe in certain cases, you cannot mimic sound or make others interpret data without them being able to hear it. 
Listening to sound representing data can require learning or inherent talent, and this learning can take a longer time than looking at visually represented data, although that depends on the type of data. 
The type of data represented also needs to be taken into account for whenever a sound or pattern is chosen. 
\enquote{It is hard to represent a spatial arrangement with sound. You need to use the appropriate display.}
Said Bruce Walker, a professor in the schools of psychology and interactive computing at Georgia Tech, running a group called the sonification lab~\cite*{Feder2012}.

To give a more concrete example of an advantage, sonification was used to study a model of an artificial heart pump, where a modulated sound was indicating pressure on a pusher plate, and a tapping sound indicated a blood cell entering threshold vorticity, and a drum indicated the opening and closing of heart valves. 
It was reported that it seemed easier to determine different things, such as the frequency of blood cells crossing and detecting whether or not the heart valves were opening or closing. 
This is in contrast to the standard method of looking at change in color on a monitor. 
Another example is sonification used to indicate seismic events caused by the fracturing of rock walls during a 3 month dig.
The engineers had a visualization to indicate these fractures, but the events were short and to address this, they added a short sound that varied in volume depending on the magnitude of the event. 
The sounds would draw more attention to periods with more seismic activity, making it easier to detect overlapping events that indicate higher risk. 
Sounds are extremely useful when variables do not appear together in the same image, or require shifting visual attention from monitor to monitor. 
From these two examples, it is clear that when dealing with data that has a variety of events or over longer periods of time, using sound can make the data easier to comprehend.


\subsubsection{Weather data represented using sound} % (fold)
\label{ssub:weather_data_represented_using_sound}

Engineers use visualizations to represent a large amount of data over complex systems, an auditory display can be useful in these cases, where the interpretant needs to keep his eyes focused on the visual data while also receiving data from an audio source. 
The quality and completeness of a fluid flow simulation was analyzed by comparing the sonified audio data generated by a simulated turbine and an actual recording of a turbine~\cite*{Barrass1999}.

Sounds can make it easier to perceive cycles and rhythm, and also abnormalities in a large amount of data. 
Another example of a visualization designed by Simon Kravis at the CSIRO was made to assist the engineers planning a water treatment, works by using a computer model. 
Chemical concentrations in the river’s water are shown by various colored segments, according to the downpour over a one year period. 
They realized that it is difficult to watch the downpour graph and the river visualization at the same time, so they decided to add sonification to represent the downpour so that an engineer may pay less visual attention to the river, while quantities of the downpour are represented by rain-like sound. 
The result of this was that after several repetitions, an engineer could learn the patterns of the rainfall over the year and can anticipate cycles in downpour or dryness. 
The sound representation is meant to bind the different sets of visualizations together and make it easier to remember, and an engineer was able to make relations between chemicals that are never seen at the same time~\cite*{Barrass1998}. 

% subsubsection weather_data_represented_using_sound (end)

% subsection advantages_and_disadvantages_of_sonification (end)


\subsection{Analogic vs Symbolic representation} % (fold)
\label{sub:analogic_vs_symbolic_representation}

There are many different ways to go about designing something that uses sonification. Methods for designing auditory displays have been classified along a spectrum between analogic and symbolic by Kramer~\cite*{Barrass1999}.
\enquote{An analogic representation is one in which there is an immediate and intrinsic correspondence between the sort of structure being represented and the representation medium.}
Meaning, there is one to one mapping between points in the data and points in representation space~\cite*{Barrass1999}. 
Examples for this include a Geiger counter or an auditory thermometer.

Its counterpart, symbolic representation, categorically denotes the thing being represented and relations within the representation do not necessarily reflect intrinsic relationships in what is being represented. 
Examples of a display like this are computer beeps, cellphone alarms and vehicle control notifications.

% subsection analogic_vs_symbolic_representation (end)


\subsection{Semiosis} % (fold)
\label{sub:semiosis}

Semiotics is the study of signs and their creation and interpretation. 
Signs can be anything from images to words, sounds and smells. 
These signs have no inherent meaning other than the one we attribute to them. 
Modern semiotics is based on the work of two major figures, Ferdinand de Saussure and Charles Sanders Peirce, a Swiss linguist and an American scientist respectively~\cite*{Vickers2012}.
In Saussure’s view the sign system is a dyadic relationship between a \enquote{signifier} and a \enquote{signified}.
A signifier is typically a sound pattern, and a signified is the meaning that signifier carries. 
An example of this is the symbol \texttt{/cat/}, which in this case is a signifier for the concept or meaning that we know as a cat. 
The link evoked between the sound pattern and the meaning is the resultant sign. 
Peirce considered this relationship to be insufficient, and he proposed another scheme in which an object referred to is represented by a \enquote{representamen} which then evokes in the mind of the listener the meaning of the sign which he refers to as the \enquote{interpretant}. 
The two theories loosely refer to one another and can be shown as such~\cite*{Vickers2012}(Figure~\ref{fig:semiosis1}):

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.5\textwidth]{images/Semiosis1.png}
    \caption{XX}
    \label{fig:semiosis1}
\end{figure}

% subsection semiosis (end)


\subsection{Sonification usage} % (fold)
\label{sub:sonification_usage}

Sonification can be split up in 4 different kinds of function types, when going by theory created by Bruce N. Walker and Michael A. Ness~\cite*{walker2011}. 
The 4 different types are as follows: 

\begin{enumerate}
    \item Alarms, alerts, and warnings
    \item Status, process, and monitoring messages
    \item Data exploration
    \item Art and entertainment
\end{enumerate}


\subsubsection*{Alerting functions} % (fold)
\label{ssub:alerting_functions}

Alerting functions refers to sound indicating that a certain event is occurring or has occurred, but it usually won’t convey descriptive data on the alerted subject. 
For example, a microwave oven makes a beep when the timer has expired, but it will not tell you if the containing substance is done or overcooked. 
Another example is the usage of calendar notifications on phones or tablets. 
These notifications will only notify you that something is going to happen by ringing or beeping. 
To further understand what the occurring sound means, a visual clue or a memory of the upcoming event is required.

The last example in this category is a fire alarm. 
A fire alarm will ring loudly to alert you that you need to move immediately, in most cases out of the building, but it does not contain any information about the location of the fire, nor does it tell you how severe the fire is.

% subsubsection alerting_functions (end)


\subsubsection*{Status and progress functions} % (fold)
\label{ssub:status_and_progress_functions}

Status and progress function has a purpose similar to that of alerting functions, but to understand the data correctly, a visual display or another form of continuous data is required to fully understand the situation at hand. 
It requires the interpretant to pay attention to small changes in the auditory feedback. 
This relates to the example given (Section~\ref{sec:preanalysis}) that we have previously discussed, where surgeons focus their eyes on the patient while listening to the heart monitors.

When designing a weather application, this could be the function to use if your goal was to have mostly visual data, only supplied by auditory data. 
However, it is important to note that when using this type of sonification, the auditory display itself is only a sub part of the data. 

% subsubsection status_and_progress_functions (end)


\subsubsection*{Data exploration functions} % (fold)
\label{ssub:data_exploration_functions}

When talking of sonification, this is the function usually referred to. 
Data exploration is meant to encode and convey information about entire sets of data, or relevant aspects of said data.
Sonification designed for data exploration is different from status or process indicators, seeing as they use sound to offer a more \enquote{holistic portrait} of the data, as opposed to condensing information to capture a momentary state~\cite*{walker2011}.

% subsubsection data_exploration_functions (end)


\subsubsection*{Art and Entertainment} % (fold)
\label{ssub:art_and_entertainment}

Since the sound producing capabilities of computers have evolved exponentially over the past years, so have the music producing of computers too. 
Sound data like sonification have been adapted more and more in to the world of music, and more artist uses data mapping included in their song or tracks. 
Since music does not really convey any form of information, it can still be included in the sonification functions, since it still conveys the expression/data of art and entertainment.

The above covers the different sonification functions, and in which cases they can be useful to alert or notify interpretants.

% subsubsection art_and_entertainment (end)


As you read above sonification have a lot of functioning abilities, as to how you can use audification without speech to notify a user what happens around them without using their eyes or hands, but sonification can also be categorized in their techniques and how you approach them. 
So far we have only explained what sonification can be used for, but not how the techniques are in practice, does it require interaction from a user or not?. 
Those approaches can be categorized in 3 categories which are (1) event based, (2) Model Based (3) Continuous.


\subsubsection*{Event-based sonification} % (fold)
\label{ssub:event_based_sonification}

The Event based approach is where the data of the sonification display is employed by the parameter mapping. 
Parameter mapping are changes in data dimension with changes in an acoustic dimension to produce a sonification. 
Parameter sees changes in data and then try to convey that data with as much as a feasible display through sounds, to show what that data means, a lot like what we want to do with data of the weather. 
The event based approaches have more or less through time, had a passive interaction possibility with brief notification like alarms and notification, but event based sonification that employ parameter mapping have the possibility to adapt to both passive and active interaction.

The sonification Handbook~\cite[363]{Hermann2011} shows several examples to sound that tells the user e.g. that their target destination is reached.
The data will read the remaining length of the journey and then intensify the sonification/sound depending on how close you are to your target.

% subsubsection event_based_sonification (end)


\subsubsection*{Model-based sonification} % (fold)
\label{ssub:analysis_model_based_sonification}

Model based differs from event based a lot, since here you need a virtual model that relies heavily on the interaction between the user and the virtual object. 
The virtual object then reads the users interaction, and converts that interaction into sonification. 

The Sonification Handbook video number 16.3~\cite*[410]{Hermann2011} shows a perfect example of model based sonification. 
You see a user with a triangular object moving around, the software then detects his movement and convert his actions in to sounds depending on how the object is moved. 
Another example of this can be a metal detector, which some people uses on the beach to scavenge forgotten objects on the beach. 
The metal detector will notify/alarm you if your metal detector senses any metal beneath its reader. 

% subsubsection analysis_model_based_sonification (end)


\subsubsection*{Continuous sonification} % (fold)
\label{ssub:continuous_sonification}

Continuous sonification is possible when data are timed series and sampled at a rate that a quasi-analog signal can be directly translated in to sound (quasi-analog signal in telecommunication is a digital signal that has been converted to a suitable form for transmission through analog channel). 
To explain it more simply is when sound waves are directly translated in to sound. 
That means the sonification happens when the software translate the data from a sound wave in to sound e.g. a Richter scale converts seismic data in to wave forms, that can then show the seismic event with a 90\% accuracy.

% subsubsection continuous_sonification (end)


The group will generally focus on the first approach of sonification (event based), since the task for our idea is to notify the user, what he/she can expect the weather condition to be for the day or notify the user that its going to rain in X minutes, and by using parameter mapping we can also change the sound output dependent on the data of the weather condition, and ideally it will make a sound that the user can understand how warm or how cold it is outside, by just hearing a sound.

% subsection sonification_usage (end)


We now know that sonification can notify you in many different ways, that some forms require a passive listener and some requires and active listener, maybe both. 
The approaches for sonification is also different depending on the task or the message that you which you convey, and that sonification can also have interaction methods that require the user to physically interact with an object to generate sonification to work. 
The group now needs to work out what kind of sonification function and techniques that need to be implemented in to our project and work from there.

% section analysis (end)
